"""
XPLIA TIER 2 & TIER 3 - Advanced Research & Experimental Features Demo

Demonstrates all 9 cutting-edge research and experimental modules:

TIER 2 (Research Excellence):
1. Meta-Learning & Few-Shot (MAML, Prototypical Networks)
2. Neuro-Symbolic AI (Rule Extraction, Logic)
3. Continual Learning (Evolution, Forgetting)
4. Bayesian Deep Learning (Uncertainty Decomposition)
5. Mixture of Experts (Expert Routing like GPT-4)
6. Recommender Systems (CF, Matrix Factorization)

TIER 3 (Experimental/Future):
7. Quantum ML (Quantum Circuits)
8. Neural Architecture Search (AutoML)
9. Neural ODEs (Continuous Models)

Author: XPLIA Team
License: MIT
"""

import numpy as np
import warnings
warnings.filterwarnings('ignore')

print("=" * 100)
print(" " * 30 + "XPLIA TIER 2 & TIER 3 FEATURES DEMO")
print(" " * 25 + "Research Excellence + Experimental Future")
print("=" * 100)

# =============================================================================
# TIER 2 - MODULE 1: META-LEARNING & FEW-SHOT
# =============================================================================

print("\n" + "=" * 100)
print("TIER 2 - MODULE 1: META-LEARNING & FEW-SHOT EXPLAINABILITY")
print("=" * 100)

from xplia.explainers.metalearning import MAMLExplainer, PrototypicalNetworkExplainer

print("\n1.1 MAML (Model-Agnostic Meta-Learning)")
print("-" * 100)

# 5-way 1-shot task
support_set = [(np.random.randn(64), c) for c in range(5)]
query = np.random.randn(64)

maml_exp = MAMLExplainer(None)
maml_result = maml_exp.explain_adaptation(support_set, query, n_adaptation_steps=5)

print(f"Task: 5-way 1-shot classification")
print(f"Adaptation steps: {len(maml_result.adaptation_steps)}")

print(f"\nAdaptation progress:")
for step in maml_result.adaptation_steps[:3]:
    print(f"  Step {step['step']}: loss={step['loss']:.4f}, grad_norm={step['gradient_norm']:.4f}")

print(f"\nMost influential support examples:")
top_influential = sorted(maml_result.prototype_influence.items(), key=lambda x: x[1], reverse=True)[:3]
for idx, influence in top_influential:
    print(f"  Example {idx}: influence={influence:.4f}")

print("\n1.2 Prototypical Networks")
print("-" * 100)

proto_exp = PrototypicalNetworkExplainer(None)
proto_result = proto_exp.explain_classification(support_set, query)

print(f"Predicted class: {proto_result['predicted_class']}")
print(f"\nDistances to class prototypes:")
for c, dist in proto_result['distances_to_prototypes'].items():
    marker = "âœ“" if c == proto_result['predicted_class'] else " "
    print(f"  {marker} Class {c}: distance={dist:.4f}")

# =============================================================================
# TIER 2 - MODULE 2: NEURO-SYMBOLIC AI
# =============================================================================

print("\n" + "=" * 100)
print("TIER 2 - MODULE 2: NEURO-SYMBOLIC AI EXPLAINABILITY")
print("=" * 100)

from xplia.explainers.neurosymbolic import RuleExtractor, LogicExplainer

print("\n2.1 Symbolic Rule Extraction")
print("-" * 100)

X_train = np.random.randn(100, 5)
y_train = (X_train[:, 0] + X_train[:, 1] > 0).astype(int)

rule_extractor = RuleExtractor(None)
rules = rule_extractor.extract_decision_rules(X_train, y_train)

print(f"Extracted {len(rules)} symbolic rules:")
for rule in rules[:3]:
    print(f"\nRule {rule['rule_id']}: IF {rule['condition']} THEN class={rule['prediction']}")
    print(f"  Confidence: {rule['confidence']:.2%}, Support: {rule['support']} samples")

print("\n2.2 First-Order Logic Explanation")
print("-" * 100)

x_test = np.random.randn(5)
prediction = 1

logic_exp = LogicExplainer()
logic_result = logic_exp.explain_as_logic(x_test, prediction)

print(f"Logic formula: {logic_result['formula']}")
print(f"Logic type: {logic_result['logic_type']}")

# =============================================================================
# TIER 2 - MODULE 3: CONTINUAL LEARNING
# =============================================================================

print("\n" + "=" * 100)
print("TIER 2 - MODULE 3: CONTINUAL LEARNING EXPLAINABILITY")
print("=" * 100)

from xplia.explainers.continual import ExplanationEvolutionTracker, CatastrophicForgettingDetector

print("\n3.1 Explanation Evolution Tracking")
print("-" * 100)

tracker = ExplanationEvolutionTracker()

for task_id in range(5):
    explanation = np.random.randn(10)
    tracker.track_explanation(task_id, explanation)

drift_result = tracker.detect_drift()

print(f"Tasks tracked: {drift_result['n_tasks_tracked']}")
print(f"Drift detected: {drift_result['drift_detected']}")
print(f"Drift magnitude: {drift_result['drift_magnitude']:.4f}")

print("\n3.2 Catastrophic Forgetting Detection")
print("-" * 100)

# Performance degraded on earlier tasks
task_performances = {0: 0.95, 1: 0.90, 2: 0.60, 3: 0.55}
current_task = 4

detector = CatastrophicForgettingDetector()
forgetting_result = detector.detect_forgetting(task_performances, current_task)

print(f"Forgetting detected: {forgetting_result['forgetting_detected']}")
print(f"Average forgetting: {forgetting_result['average_forgetting']:.2%}")
print(f"\nPer-task forgetting:")
for task, forget in forgetting_result['per_task_forgetting'].items():
    print(f"  Task {task}: {forget:.2%}")

# =============================================================================
# TIER 2 - MODULE 4: BAYESIAN DEEP LEARNING
# =============================================================================

print("\n" + "=" * 100)
print("TIER 2 - MODULE 4: BAYESIAN DEEP LEARNING EXPLAINABILITY")
print("=" * 100)

from xplia.explainers.bayesian import UncertaintyDecomposer, BayesianFeatureImportance

print("\n4.1 Uncertainty Decomposition (Aleatoric vs Epistemic)")
print("-" * 100)

x_input = np.random.randn(10)

decomposer = UncertaintyDecomposer(None, n_samples=100)
unc_result = decomposer.decompose_uncertainty(x_input)

print(f"Total uncertainty: {unc_result['total_uncertainty']:.4f}")
print(f"  Epistemic (model) uncertainty: {unc_result['epistemic_uncertainty']:.4f} ({unc_result['epistemic_ratio']:.1%})")
print(f"  Aleatoric (data) uncertainty: {unc_result['aleatoric_uncertainty']:.4f}")
print(f"\nPrediction: {unc_result['predictions_mean']:.4f} Â± {unc_result['predictions_std']:.4f}")

print("\n4.2 Bayesian Feature Importance with Credible Intervals")
print("-" * 100)

X_data = np.random.randn(100, 10)

bay_fi = BayesianFeatureImportance(None)
importance_result = bay_fi.compute_importance_with_uncertainty(X_data, n_samples=100)

print(f"Feature importance with 95% credible intervals:")
for i in range(5):
    mean = importance_result['mean_importance'][i]
    lower = importance_result['credible_interval_95']['lower'][i]
    upper = importance_result['credible_interval_95']['upper'][i]
    print(f"  Feature {i}: {mean:6.3f} [{lower:6.3f}, {upper:6.3f}]")

# =============================================================================
# TIER 2 - MODULE 5: MIXTURE OF EXPERTS
# =============================================================================

print("\n" + "=" * 100)
print("TIER 2 - MODULE 5: MIXTURE OF EXPERTS EXPLAINABILITY (like GPT-4)")
print("=" * 100)

from xplia.explainers.moe import ExpertRoutingExplainer, ExpertSpecializationAnalyzer

print("\n5.1 Expert Routing Explanation")
print("-" * 100)

x_input = np.random.randn(512)
n_experts = 8

routing_exp = ExpertRoutingExplainer(None, n_experts=n_experts)
routing_result = routing_exp.explain_routing(x_input)

print(f"Routing strategy: {routing_result['routing_strategy']}")
print(f"Total experts: {routing_result['n_experts']}, Selected: {routing_result['top_k']}")

print(f"\nSelected experts:")
for exp in routing_result['selected_experts']:
    print(f"  Expert {exp['expert_id']}: gate_score={exp['gate_score']:.3f} (specializes in {exp['specialization']})")

print("\n5.2 Expert Specialization Analysis")
print("-" * 100)

analyzer = ExpertSpecializationAnalyzer()
eval_data = np.random.randn(100, 20)
specializations = analyzer.analyze_specialization(n_experts, eval_data)

print(f"Expert specializations:")
for expert_id, spec in list(specializations.items())[:4]:
    print(f"\n  Expert {expert_id}:")
    print(f"    Activation rate: {spec['activation_rate']:.2%}")
    print(f"    Expertise score: {spec['expertise_score']:.2f}")

# =============================================================================
# TIER 2 - MODULE 6: RECOMMENDER SYSTEMS
# =============================================================================

print("\n" + "=" * 100)
print("TIER 2 - MODULE 6: RECOMMENDER SYSTEM EXPLAINABILITY")
print("=" * 100)

from xplia.explainers.recommender import CollaborativeFilteringExplainer, MatrixFactorizationExplainer

print("\n6.1 Collaborative Filtering Explanation")
print("-" * 100)

n_users, n_items = 100, 50
user_item_matrix = np.random.randint(0, 6, (n_users, n_items))

cf_exp = CollaborativeFilteringExplainer(user_item_matrix)
cf_result = cf_exp.explain_recommendation(user_id=5, item_id=10, k_similar_users=5)

print(f"Why recommend item {cf_result['item_id']} to user {cf_result['user_id']}?")
print(f"Explanation: {cf_result['explanation']}")

print(f"\nEvidence from similar users:")
for evidence in cf_result['similar_users_who_liked'][:3]:
    print(f"  User {evidence['similar_user_id']} (similarity: {evidence['similarity']:.3f}) rated {evidence['rating']:.1f}/5")

print("\n6.2 Matrix Factorization Explanation")
print("-" * 100)

k = 20
user_factors = np.random.randn(n_users, k)
item_factors = np.random.randn(n_items, k)

mf_exp = MatrixFactorizationExplainer(user_factors, item_factors)
mf_result = mf_exp.explain_prediction(user_id=5, item_id=10)

print(f"Predicted rating: {mf_result['predicted_rating']:.2f}/5")
print(f"Top contributing latent factors: {mf_result['top_contributing_factors']}")

# =============================================================================
# TIER 3 - MODULE 7: QUANTUM ML
# =============================================================================

print("\n" + "=" * 100)
print("TIER 3 - MODULE 7: QUANTUM MACHINE LEARNING EXPLAINABILITY")
print("=" * 100)

from xplia.explainers.quantum import QuantumCircuitExplainer

print("\nQuantum Circuit Explanation")
print("-" * 100)

quantum_circuit = {
    'n_qubits': 4,
    'gates': [
        {'type': 'H', 'qubits': [0]},
        {'type': 'CNOT', 'qubits': [0, 1]},
        {'type': 'RX', 'qubits': [2]},
        {'type': 'CNOT', 'qubits': [1, 2]},
        {'type': 'RY', 'qubits': [3]}
    ]
}

qc_exp = QuantumCircuitExplainer()
qc_result = qc_exp.explain_circuit(quantum_circuit)

print(f"Circuit: {qc_result['n_qubits']} qubits, {qc_result['n_gates']} gates, depth {qc_result['circuit_depth']}")
print(f"Entanglement measure: {qc_result['entanglement_measure']:.3f}")

print(f"\nGate effects:")
for gate_exp in qc_result['gate_explanations'][:4]:
    print(f"  {gate_exp['gate_type']} on qubits {gate_exp['qubits']}: {gate_exp['effect']}")

# =============================================================================
# TIER 3 - MODULE 8: NEURAL ARCHITECTURE SEARCH
# =============================================================================

print("\n" + "=" * 100)
print("TIER 3 - MODULE 8: NEURAL ARCHITECTURE SEARCH EXPLAINABILITY")
print("=" * 100)

from xplia.explainers.nas import ArchitectureExplainer

print("\nArchitecture Selection Explanation")
print("-" * 100)

discovered_arch = {
    'components': [
        {'type': 'Conv2D', 'params': {'filters': 64, 'kernel': 3}},
        {'type': 'BatchNorm', 'params': {}},
        {'type': 'ReLU', 'params': {}},
        {'type': 'ResBlock', 'params': {'depth': 2}},
        {'type': 'MaxPool', 'params': {'pool_size': 2}}
    ]
}

nas_exp = ArchitectureExplainer()
nas_result = nas_exp.explain_architecture(discovered_arch)

print(f"Architecture score: {nas_result['architecture_score']:.3f}")
print(f"Why selected: {nas_result['why_selected']}")
print(f"Search space explored: {nas_result['search_space_explored']}")

print(f"\nComponent contributions:")
for comp in nas_result['components'][:4]:
    print(f"  {comp['component_type']}: perf={comp['performance_contribution']:.2f}, efficiency={comp['efficiency_score']:.2f}")

# =============================================================================
# TIER 3 - MODULE 9: NEURAL ODEs
# =============================================================================

print("\n" + "=" * 100)
print("TIER 3 - MODULE 9: NEURAL ODEs EXPLAINABILITY")
print("=" * 100)

from xplia.explainers.neuralodes import NeuralODEExplainer

print("\nNeural ODE Trajectory Explanation")
print("-" * 100)

initial_state = np.random.randn(5)
time_points = np.linspace(0, 1, 20)

node_exp = NeuralODEExplainer()
node_result = node_exp.explain_trajectory(initial_state, time_points)

print(f"Time points: {node_result['n_timepoints']}")
print(f"Dynamics type: {node_result['dynamics_type']}")
print(f"Average velocity: {node_result['avg_velocity']:.4f}")
print(f"Max velocity: {node_result['max_velocity']:.4f}")
print(f"Total trajectory length: {node_result['trajectory_length']:.4f}")

# =============================================================================
# SUMMARY
# =============================================================================

print("\n" + "=" * 100)
print("TIER 2 & TIER 3 DEMO COMPLETE!")
print("=" * 100)

print("\nTIER 2 (Research Excellence):")
print("  âœ… Meta-Learning & Few-Shot (MAML, Prototypical Networks)")
print("  âœ… Neuro-Symbolic AI (Symbolic Rules, First-Order Logic)")
print("  âœ… Continual Learning (Evolution Tracking, Forgetting Detection)")
print("  âœ… Bayesian Deep Learning (Uncertainty Decomposition)")
print("  âœ… Mixture of Experts (Expert Routing like GPT-4)")
print("  âœ… Recommender Systems (CF, Matrix Factorization)")

print("\nTIER 3 (Experimental Future):")
print("  âœ… Quantum ML (Quantum Circuit Explainability)")
print("  âœ… Neural Architecture Search (AutoML Explanations)")
print("  âœ… Neural ODEs (Continuous Dynamics)")

print("\nðŸŽ“ XPLIA now has RESEARCH-LEVEL + EXPERIMENTAL CUTTING-EDGE features!")
print("=" * 100)
