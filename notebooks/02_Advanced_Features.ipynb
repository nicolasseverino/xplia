{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ XPLIA - Fonctionnalit√©s Avanc√©es\n",
    "\n",
    "Ce notebook explore les fonctionnalit√©s avanc√©es de XPLIA:\n",
    "- Comparaison de mod√®les\n",
    "- Diff√©rentes m√©thodes d'explicabilit√©\n",
    "- Performance tracking\n",
    "- Validation et robustesse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from xplia.core import ModelFactory, ExplainerFactory, VisualizerFactory\n",
    "from xplia.utils import measure_performance, validate_input, validate_model\n",
    "\n",
    "print(\"‚úì Setup complet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Comparaison de Plusieurs Mod√®les"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G√©n√©rer des donn√©es\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=20,\n",
    "    n_informative=15,\n",
    "    n_redundant=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Dataset: {X.shape[0]} samples, {X.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entra√Æner plusieurs mod√®les\n",
    "models = {\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'GradientBoosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'LogisticRegression': LogisticRegression(max_iter=1000, random_state=42)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Mod√®le: {name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Entra√Ænement avec mesure de performance\n",
    "    with measure_performance(f\"Training {name}\", track_memory=True) as metrics:\n",
    "        model.fit(X_train, y_train)\n",
    "    \n",
    "    # √âvaluation\n",
    "    train_score = model.score(X_train, y_train)\n",
    "    test_score = model.score(X_test, y_test)\n",
    "    \n",
    "    # D√©tection et recommandation\n",
    "    model_type = ModelFactory.detect_model_type(model)\n",
    "    recommended = ExplainerFactory.get_recommended_method(model)\n",
    "    \n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'train_score': train_score,\n",
    "        'test_score': test_score,\n",
    "        'training_time': metrics['elapsed_time'],\n",
    "        'memory_used': metrics['memory_used'],\n",
    "        'model_type': model_type,\n",
    "        'recommended_method': recommended\n",
    "    }\n",
    "    \n",
    "    print(f\"Train accuracy: {train_score:.4f}\")\n",
    "    print(f\"Test accuracy:  {test_score:.4f}\")\n",
    "    print(f\"Training time:  {metrics['elapsed_time']:.3f}s\")\n",
    "    print(f\"Memory used:    {metrics['memory_used']:.2f}MB\")\n",
    "    print(f\"Model type:     {model_type}\")\n",
    "    print(f\"Recommended:    {recommended}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tableau comparatif\n",
    "comparison_df = pd.DataFrame([\n",
    "    {\n",
    "        'Model': name,\n",
    "        'Train Acc': f\"{r['train_score']:.4f}\",\n",
    "        'Test Acc': f\"{r['test_score']:.4f}\",\n",
    "        'Time (s)': f\"{r['training_time']:.3f}\",\n",
    "        'Memory (MB)': f\"{r['memory_used']:.2f}\",\n",
    "        'Recommended Method': r['recommended_method']\n",
    "    }\n",
    "    for name, r in results.items()\n",
    "])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARAISON DES MOD√àLES\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Validation et Robustesse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de validation des entr√©es\n",
    "print(\"Test de validation des entr√©es:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Diff√©rents formats d'entr√©e\n",
    "test_inputs = [\n",
    "    (\"NumPy array\", np.array([[1, 2, 3], [4, 5, 6]])),\n",
    "    (\"Pandas DataFrame\", pd.DataFrame({'a': [1, 2], 'b': [3, 4], 'c': [5, 6]})),\n",
    "    (\"Liste Python\", [[1, 2, 3], [4, 5, 6]]),\n",
    "]\n",
    "\n",
    "for name, data in test_inputs:\n",
    "    validated = validate_input(data)\n",
    "    print(f\"‚úì {name:20s} -> shape: {validated.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de validation des mod√®les\n",
    "print(\"\\nTest de validation des mod√®les:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for name, result in results.items():\n",
    "    model = result['model']\n",
    "    try:\n",
    "        validate_model(model, required_methods=['fit', 'predict', 'predict_proba'])\n",
    "        print(f\"‚úì {name:20s} - Toutes les m√©thodes pr√©sentes\")\n",
    "    except ValueError as e:\n",
    "        print(f\"‚úó {name:20s} - {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. M√©thodes d'Explicabilit√© Disponibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lister toutes les m√©thodes\n",
    "methods = ExplainerFactory.list_available_methods()\n",
    "\n",
    "print(\"M√©thodes d'explicabilit√© disponibles:\")\n",
    "print(\"=\"*50)\n",
    "for i, method in enumerate(methods, 1):\n",
    "    print(f\"{i:2d}. {method}\")\n",
    "\n",
    "print(f\"\\nTotal: {len(methods)} m√©thodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Types de Visualisations Disponibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lister tous les types de graphiques\n",
    "charts = VisualizerFactory.list_available_charts()\n",
    "\n",
    "print(\"Types de graphiques disponibles:\")\n",
    "print(\"=\"*50)\n",
    "for i, chart in enumerate(charts, 1):\n",
    "    print(f\"{i:2d}. {chart}\")\n",
    "\n",
    "print(f\"\\nTotal: {len(charts)} types de graphiques\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommandations de graphiques par type d'explication\n",
    "explanation_types = [\n",
    "    'feature_importance',\n",
    "    'shap_values',\n",
    "    'lime_weights',\n",
    "    'partial_dependence',\n",
    "    'interaction',\n",
    "    'counterfactual',\n",
    "    'uncertainty'\n",
    "]\n",
    "\n",
    "print(\"\\nRecommandations de graphiques:\")\n",
    "print(\"=\"*50)\n",
    "for exp_type in explanation_types:\n",
    "    recommended = VisualizerFactory.get_recommended_chart(exp_type)\n",
    "    print(f\"{exp_type:25s} -> {recommended}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Workflow Complet avec le Meilleur Mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S√©lectionner le meilleur mod√®le (par test accuracy)\n",
    "best_model_name = max(results.items(), key=lambda x: x[1]['test_score'])[0]\n",
    "best_model = results[best_model_name]['model']\n",
    "\n",
    "print(f\"Meilleur mod√®le: {best_model_name}\")\n",
    "print(f\"Test accuracy: {results[best_model_name]['test_score']:.4f}\")\n",
    "\n",
    "# Cr√©er l'explainer\n",
    "recommended_method = results[best_model_name]['recommended_method']\n",
    "print(f\"\\nCr√©ation de l'explainer avec m√©thode: {recommended_method}\")\n",
    "\n",
    "from xplia.core import create_explainer\n",
    "explainer = create_explainer(best_model, method='unified')\n",
    "print(f\"‚úì Explainer cr√©√©: {type(explainer).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G√©n√©rer des explications\n",
    "with measure_performance(\"G√©n√©ration explications\") as metrics:\n",
    "    explanation = explainer.explain(X_test[:10])\n",
    "\n",
    "print(f\"\\n‚úì Explications g√©n√©r√©es en {metrics['elapsed_time']:.3f}s\")\n",
    "print(f\"  M√©moire utilis√©e: {metrics['memory_used']:.2f}MB\")\n",
    "\n",
    "# Afficher les top features\n",
    "print(\"\\nTop 5 Features Importantes:\")\n",
    "print(\"=\"*50)\n",
    "for i, fi in enumerate(explanation.feature_importances[:5], 1):\n",
    "    print(f\"{i}. Feature {fi.feature_name:10s} : {fi.importance_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Conclusion\n",
    "\n",
    "Ce notebook a d√©montr√©:\n",
    "- ‚úÖ Comparaison de plusieurs mod√®les\n",
    "- ‚úÖ Mesure de performance automatique\n",
    "- ‚úÖ Validation robuste des entr√©es\n",
    "- ‚úÖ Recommandations intelligentes\n",
    "- ‚úÖ Workflow complet d'explicabilit√©\n",
    "\n",
    "### Prochaines √©tapes:\n",
    "- Essayer avec vos propres donn√©es\n",
    "- Explorer d'autres m√©thodes d'explicabilit√©\n",
    "- Cr√©er des visualisations personnalis√©es"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
